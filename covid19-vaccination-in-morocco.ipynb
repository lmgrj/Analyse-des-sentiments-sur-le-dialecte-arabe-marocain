{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>Arabic Social Media Sentiment Analysis</center></h1> \n\n<center><h1>Moroccan Arabic Dialect classification </center><h1>\n\n<center><h1> WISD-----Web Mining </center><h1>\n\n\n---\nBy: **Lamgarraj Mohamed & Zakaria elghamch**\n\n\n---\n\n","metadata":{"id":"sY3bsXjI6zUc"}},{"cell_type":"markdown","source":"# **Loading data**","metadata":{"id":"JH95Q_KS2wRs"}},{"cell_type":"markdown","source":"Training data","metadata":{"id":"3zEYhjyB-yE7"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"IOgLUxf_SmAk","outputId":"a58c6777-b799-442f-fa7e-580a852e0e54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('/content/drive/MyDrive/WeBMining/train.csv', sep=',',encoding='utf-8')\ndata.head","metadata":{"id":"P8CE8xSfx1oY","outputId":"d283075c-51a7-4ee4-ecb1-bd3e42fad5d1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data to predict","metadata":{"id":"EdamoRv1-2L_"}},{"cell_type":"code","source":"test=pd.read_csv('/content/drive/MyDrive/WeBMining/test_stage1.csv', sep=',',encoding='utf-8')\ntest.head","metadata":{"id":"gRLB9HMftPQx","outputId":"c3a9fa13-6a74-4ba5-8478-afd606d4c657"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **prétraitement version:1**","metadata":{"id":"oyVFLuJ-WoEi"}},{"cell_type":"code","source":"from urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup\nfrom google.colab import drive\nimport csv\nimport re\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.stem.isri import ISRIStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize","metadata":{"id":"bATzZgBfWr6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n  stopwords = nltk.corpus.stopwords.words('arabic')\nexcept:\n  nltk.download('stopwords')\n  stopwords = nltk.corpus.stopwords.words('arabic')\n\nsw_file = urlopen(\"https://raw.githubusercontent.com/mohataher/arabic-stop-words/master/list.txt\").read().decode('utf-8')\nsw = [x for x in sw_file.split(\"\\n\")]\nstopwords = set(stopwords + sw)\n\ndef remove_repeating_char(text):\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)# keep only 1 repeat\n\narabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n\ndef remove_lt2(text):\n  return re.sub(r'\\b\\w{,2}\\b', '',text)\n\ndef remove_diacritics(text):\n    return re.sub(arabic_diacritics, '', text)\n    \ndef remove_punctuations(text):\n    arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n    #english_punctuations = string.punctuation\n    #all_punctuations = set(arabic_punctuations + english_punctuations)\n    for p in arabic_punctuations:\n        if p in text:\n            text = text.replace(p, '')\n    return text\n\ndef remove_mention(text):\n    return re.sub(r'@\\S+', '', text)\n\ndef allow_only_ar(text):\n    return re.sub(r'[^\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+', ' ', text)\n\ndef hashtag_match(match_object):\n        return match_object.group(1).replace('_', ' ')\n\ndef normalize_hashtag(text):\n    return re.sub(r'#(\\S+)', hashtag_match, text)\n\ndef remove_stopwords(text):\n  word_tokens = wordpunct_tokenize(text)\n  filtered_sentence = [w for w in word_tokens if not w in stopwords]\n  return \" \".join(filtered_sentence)\n\ndef process_text(text):\n    \n    clean_text = normalize_hashtag(text)\n    clean_text = remove_mention(clean_text)\n    clean_text = allow_only_ar(clean_text)\n    clean_text = remove_punctuations(clean_text)\n    clean_text = remove_diacritics(clean_text)\n    clean_text = remove_repeating_char(clean_text)\n    clean_text = remove_lt2(clean_text)\n    clean_text = remove_stopwords(clean_text)\n    \n    tokens = []\n    st = ISRIStemmer()\n    for w in wordpunct_tokenize(clean_text):\n      tokens.append(st.stem(w))\n    return tokens","metadata":{"id":"9NDuqoCjyt5C","outputId":"cff0343a-0b65-4c0b-cc03-1b1ca9aba9da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindata = []\nfor line in data[\"comment\"]:\n   word_list = process_text(line)\n   traindata.append(word_list)","metadata":{"id":"-cAXbppE1NBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictdata = []\nfor line in test[\"comment\"]:\n   word_list = process_text(line)\n   predictdata.append(word_list)","metadata":{"id":"65SmThz31QTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=data[\"label\"]","metadata":{"id":"q2gK2ii8i5yA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y)","metadata":{"id":"DZcLM4u3m5xR","outputId":"3c2b7a23-f585-4683-bd3a-f9167e1b7ead"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=np.array(y)","metadata":{"id":"SxmDPPCSi-co"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Prétraitement version:2 (by hanane)**","metadata":{"id":"yiOr28Edtaxb"}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n#stop_words = stopwords.words('arabic')\nstp = open(\"/content/drive/MyDrive/WeBMining/list.txt\", \"r\",encoding='utf-8')\nstops=stp.read()\nimport re\n#\nimport string\ntranslator = str.maketrans('', '', string.punctuation)\n\n\n#Stemming:\n#Arabic Light Stemming\nfrom nltk.stem.isri import ISRIStemmer","metadata":{"id":"Yi7aFDH-1h1Y","outputId":"dfcf86b1-49a1-417b-910d-b81aaca4f420"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stops.split()","metadata":{"id":"o9PPNu2h1rN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeStopWords(text):\n    #word_tokens = word_tokenize(text) \n    #filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n    #text = ' '.join([i for i in filtered_sentence])\n    #return text\n    stop_words=[]\n    #stop_words=['متفاءيل','فعال','امشي','التعجيل','انا مع فرض التلقيح','انا مع','التدابير الاحترازية','مع','لقح','فرض ','عزء ','صحي','حجر','صحي','حجر ','خيف','كمم','كلخ','بلد','وطن','تضر','روبا','صرح ','روبا','حمي','خوي','ورس','ورس ','وطن','لور','رجع','دفع','نحم ','بلد','رسي','ندر','لقح','لحل','لحل','حمد ','لله','كنا','مع','المناسب','حل','طبيع','الطبيعية','ازمة','حمي']\n    stop_words=['خرف' , 'عالميه', 'غاندير' ,'لخرافة' ,'ملك' ,'بد' , 'العلماء' , 'لسنا' ,'والعلماء','بفضل', 'شجعو','خرف','يشككون','يشكك']\n    j=0\n    word_tokens = word_tokenize(text) \n    \n    for w in word_tokens:\n        if w in stop_words:\n            word_tokens = ['انا','مع']\n            #word_tokens[1] = 'مع'\n    \n    filtered_sentence = [w for w in word_tokens]  \n    text = ' '.join([i for i in filtered_sentence])\n    return text\n\ndef NormalizeArabic(text):\n    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    return text\n\ndef arabic_diacritics(text):\n    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    text = re.sub(arabic_diacritics, '', text)\n    return text\n\ndef removeNumbers(text):\n    \"\"\" Removes integers \"\"\"\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\ndef stemming(text):\n    st = ISRIStemmer()\n    stemmed_words = []\n    word_tokens = word_tokenize(text) \n    for w in word_tokens:\n        stemmed_words.append(st.stem(w))\n    stemmed_words = \" \".join(stemmed_words)\n    return stemmed_words\n\ndef remove_english_characters(text):\n        return re.sub(r'[a-zA-Z]+', '', text)","metadata":{"id":"F0noh6R-tZ9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in data.iterrows():\n    #row['comment'] = removeStopWords(row['comment'])\n    row['comment'] = NormalizeArabic(row['comment'])\n    #row['comment'] = arabic_diacritics(row['comment'])\n    row['comment'] = removeNumbers(row['comment'])\n    row['comment'] = row['comment'].translate(translator)\n    row['comment'] = stemming(row['comment'])\n    new_df = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n    data.update(new_df)","metadata":{"id":"a0uabUl70QyG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindata2=data","metadata":{"id":"oX9DUyCJBts5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in test.iterrows():\n    row['comment'] = removeStopWords(row['comment'])\n    row['comment'] = NormalizeArabic(row['comment'])\n    #row['comment'] = arabic_diacritics(row['comment'])\n    row['comment'] = removeNumbers(row['comment'])\n    row['comment'] = row['comment'].translate(translator)\n    row['comment'] = stemming(row['comment'])\n    new_test = pd.DataFrame({'comment': [row['comment']]}, index=[index])\n    test.update(new_test)","metadata":{"id":"Np0C8xHZ0Ucp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictdata2=test","metadata":{"id":"rl8iDfrMBy9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Wordcloud**","metadata":{"id":"QRae4J_iW2ey"}},{"cell_type":"code","source":"try:\n  from ar_wordcloud import ArabicWordCloud\n  from ar_wordcloud.utils import read_mask_image\nexcept:\n  !pip install ar_wordcloud\n  from ar_wordcloud import ArabicWordCloud\n  from ar_wordcloud.utils import read_mask_image\n\nfrom imageio import imread\n\nimage = imread('https://i.imgur.com/cOegrUM.png')\n\nmask_img = np.array(image) \n\nsentences  = data[\"comment\"].tolist()\nsentences_as_one_string = \" \".join(sentences)\n\nsentences_as_one_string = allow_only_ar(sentences_as_one_string)\nsentences_as_one_string = remove_punctuations(sentences_as_one_string)\n#sentences_as_one_string = remove_stopwords(sentences_as_one_string)\n\nawc = ArabicWordCloud(background_color=\"white\",mask=mask_img)\nplt.figure(figsize=(17,17))\nwc = awc.from_text(sentences_as_one_string)\nwc.to_file('may_wc.png')\nplt.imshow(wc)\nplt.title(\"May's word cloud\")","metadata":{"id":"ikLHIybJzCM5","outputId":"ca9d693d-1f67-40af-fb65-f9d88320d8cc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"Ws8WrwGXXKx4"}},{"cell_type":"code","source":"","metadata":{"id":"f6PahwSb0Yl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"DVHDfxAN0aVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"id":"ipd2OfG00iLY","outputId":"e859910c-0b6d-4ac0-be86-c2069836c6c7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TFIDF**","metadata":{"id":"R3xaW2lbF7kR"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"id":"WccMY7r9GJ_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf1=TfidfVectorizer(sublinear_tf=True,min_df=1,encoding='latin-1',ngram_range=(0,1))","metadata":{"id":"WZJO_nkMCgaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n                         binary=False,sublinear_tf=True)","metadata":{"id":"WxZuX3BHF_A9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf2 = TfidfVectorizer(ngram_range=(1, 2),\n                         sublinear_tf=True,encoding='latin_1')","metadata":{"id":"kW6TzgV1BWmt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf = tf_idf.fit_transform(traindata2[\"comment\"])\nX_val_tfidf = tf_idf.transform(predictdata2[\"comment\"])","metadata":{"id":"y2x7EocvHJIR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf1 = tf_idf1.fit_transform(traindata2[\"comment\"])\nX_val_tfidf1 = tf_idf1.transform(predictdata2[\"comment\"])","metadata":{"id":"mD4xJGUHGn7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf2 = tf_idf2.fit_transform(traindata2[\"comment\"])\nX_val_tfidf2 = tf_idf2.transform(predictdata2[\"comment\"])","metadata":{"id":"cUVbgdoyGp0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf2.shape","metadata":{"id":"qKZtkxLiSe3v","outputId":"03f4e681-942f-41c2-8382-602cb471a9cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_tfidf2.shape","metadata":{"id":"xOSfnLhlSnYH","outputId":"367ffb00-9983-419a-9ec0-e4a9ddf6196a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf.shape","metadata":{"id":"Lphlmr2lHzmw","outputId":"0dec11de-e8bc-4778-c861-fc68761279b1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_tfidf.shape","metadata":{"id":"WmgFRTOjH6ua","outputId":"d62220ad-d4c4-4a90-96a4-044c6aa0a251"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_traintf, X_testtf, y_traintf, y_testtf = train_test_split(X_train_tfidf, y, test_size=0.2, random_state=13)\nX_traintf1, X_testtf1, y_traintf1, y_testtf1 = train_test_split(X_train_tfidf1, y, test_size=0.2, random_state=13)\nX_traintf2, X_testtf2, y_traintf2, y_testtf2 = train_test_split(X_train_tfidf2, y, test_size=0.2, random_state=13)","metadata":{"id":"3cYOojKnKnSK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Count Vectorizer**","metadata":{"id":"54dh0sPl3Afz"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\n\nXPros1 = CountVectorizer(analyzer = process_text, dtype=\"uint8\",max_features=31).fit_transform(traindata).toarray()\nX_predict1 = CountVectorizer(analyzer = process_text, dtype=\"uint8\",max_features=31).fit_transform(predictdata).toarray()\nXPros2 = CountVectorizer(analyzer = process_text, dtype=\"uint8\",max_features=784).fit_transform(traindata2[\"comment\"]).toarray()\nX_predict2 = CountVectorizer(analyzer = process_text, dtype=\"uint8\",max_features=784).fit_transform(predictdata2[\"comment\"]).toarray()\n#y = df[\"vote\"].apply(lambda x: 'positive' if x >= 0 else 'negative')\n#y=data[\"label\"]\n\nX_train, X_test, y_train, y_test = train_test_split(XPros1, y, test_size=0.2, random_state=13)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(XPros2, y, test_size=0.2, random_state=13)\n","metadata":{"id":"z6KwLw4QzO9O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Word2vec**","metadata":{"id":"40Um0q5y9Dw8"}},{"cell_type":"code","source":"class WordVecVectorizer(object):\n    def __init__(self, word2vec, max_len=50):\n        self.word2vec = word2vec\n        self.dim = word2vec.vector_size\n        self.max_len = max_len\n\n    def fit(self, X, y):\n        return self    \n\n    def transform(self, X):\n        \"\"\"\n        Transforms a document to one vector of a size = word2vec.vector_size (dim), \n        by taking the mean of th vectors of every word.\n        The output size will be of shape (X.shape[0], self.dim).\n        \"\"\"\n        return np.array([\n            np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec]\n                    or [1e-12 * np.random.normal(scale=0.6, size=self.dim)], axis=0)\n            for texts in X\n        ])\n        \n    def instance_transform(self, X):\n        \"\"\"\n        Transforms a document to a 3D array based on the word2vec vector transformation of every instance (word),\n        while the words that exceeds the maximum allowable length (max_len) will be removed.\n        If the sentance is less than max_len, it will be padded with zeros.\n        The output size will be of shape (X.shape[0], self.max_len, self.dim).\n        \"\"\"\n        return np.array([\n            [self.word2vec[w] if w in self.word2vec else 1e-12 * np.random.normal(scale=0.6, size=self.dim) for i, w in enumerate(texts.split()) if i<self.max_len] + [1e-12 * np.random.normal(scale=0.6, size=self.dim)]*(self.max_len-min(self.max_len, len(texts.split())))\n            for texts in X\n        ])","metadata":{"id":"LN6Uc2MqRouR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_text","metadata":{"id":"Cy78Z8rFWyvE","outputId":"68d3b79c-ebdd-4501-ef07-858c0e82a33d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = []\nfor line in data[\"comment\"]:\n   word_list = process_text(line)\n   corpus.append(word_list)","metadata":{"id":"O-QmKCijmV4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[0:1]","metadata":{"id":"MbhY0rlunrAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nw2v_model=gensim.models.Word2Vec( sentences=corpus,size=300,window=10,min_count=1)","metadata":{"id":"mncVqZLQ9HqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = list(w2v_model.wv.vocab)\nprint(words)","metadata":{"id":"WRFXhljMp-ZA","outputId":"53642acd-427f-4b02-84ec-bc0968b3a8cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_w2V=w2v_model.f","metadata":{"id":"uPfpVVXE8WNA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SDGclassifier**","metadata":{"id":"jepT0XM4FgCE"}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nmodel = SGDClassifier(random_state=0, class_weight='balanced',\n                    loss='log', penalty='elasticnet')\ngrid = {\n    'alpha': [10 ** x for x in range(-6, 1)],\n    'l1_ratio': [0, 0.05, 0.1, 0.5, 0.8, 1],\n}\n# define grid search\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train_tfidf1, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"id":"6THyYqq4Fjyo","outputId":"918f4880-ccd1-4c4a-9ea5-5aedf883f85f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgdmodel=SGDClassifier(random_state=0, class_weight='balanced',\n                    loss='log', penalty='elasticnet',alpha= 1e-05, l1_ratio= 0.05)\nsgdmodel.fit(X_train_tfidf1, y)\nsgdlabels=sgdmodel.predict(X_val_tfidf1)","metadata":{"id":"idlq2qQ8KBkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sgdlabels","metadata":{"id":"Pn6rg2caLqNV","outputId":"d7207a3d-ca8e-43e6-962c-d66bc89f4453"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Validation labels**","metadata":{"id":"j_pzix98D1lK"}},{"cell_type":"code","source":"l=[]\nfor i in range(120):\n  l.append(1)\nfor i in range(120):\n  l.append(0)","metadata":{"id":"OU6jQya_D0qN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(labels, l))","metadata":{"id":"0NCLIuaaL5HN","outputId":"d9132ad2-3e14-4b10-85e4-41657a39a333"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_list= [i for i in range(1,241)]\ndf = pd.DataFrame({'ID':id_list , 'label':labels })#Nblabels\ndf.to_excel('/content/Lamgarraj.xlsx',index=False)","metadata":{"id":"CqffiPoaMajk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_xls = pd.read_excel('/content/Lamgarraj.xlsx', index_col=None)\ndata_xls.to_csv('/content/LamgarrajZaki.csv', encoding='utf-8',index=False)","metadata":{"id":"4yxrXiVnMwRA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **NBclassifier**","metadata":{"id":"yfCXVaRE3JNl"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nNB_classifier = MultinomialNB()\nNB_classifier.fit(X_traintf2,y_traintf2)\ny_predict_test = NB_classifier.predict(X_testtf2)\ncm = confusion_matrix(y_testtf2,y_predict_test)\nsns.heatmap(cm, annot=True, xticklabels = ['Negative', 'Positive'],yticklabels=['Negative','Positive'])\nprint(classification_report(y_testtf2,y_predict_test))","metadata":{"id":"IomDNaqxzTnp","outputId":"83a0c7f5-ee9d-4270-bf5f-056606f89e19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_classifier.fit(X_train_tfidf2,y)","metadata":{"id":"F8IlCootP9mv","outputId":"f3f4189d-67e8-47b9-a9f5-42bfcdf6bd47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Nblabels = NB_classifier.predict(X_val_tfidf2)","metadata":{"id":"4TfQz6mtQECy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Nblabels","metadata":{"id":"VQQcGjI3QHkO","outputId":"d756d05c-d68f-4430-95b9-d9ba6370224f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **SVM**","metadata":{"id":"ym1HG6SM2q4J"}},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# define model and parameters\nmodel = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [30, 10, 1.0]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train_tfidf2, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"id":"1ISkHDX4h8dW","outputId":"0573c79f-6039-43f0-ca8c-a57ea401228d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvm1=SVC(C= 1, gamma='scale', kernel='sigmoid')","metadata":{"id":"9RONFjGgk6uA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm1.fit(X_train_tfidf2,y)","metadata":{"id":"ha9c5cFvK4ZL","outputId":"8c383ff0-0069-485a-ac39-1e96ffee9d25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=svm1.predict(X_val_tfidf2)","metadata":{"id":"NopkVeJmj4dL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"id":"s_5VqzBUkDBq","outputId":"ef387704-ddce-4089-dfef-793ab5bf19bf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Linear SVM**","metadata":{"id":"WSh-4LCGHuy6"}},{"cell_type":"code","source":"from sklearn import svm\nlsvm = svm.LinearSVC(class_weight='balanced')\nlsvm.fit(X_train_tfidf2,y)","metadata":{"id":"gJJDN6dpHxb1","outputId":"c9d762e8-9d1b-458e-b6b3-c3e4af5d1369"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lsvmlabels=lsvm.predict(X_val_tfidf2)","metadata":{"id":"YmjUeKL2IHBJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tfidf","metadata":{"id":"XeRsCFJuK_jr"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\ny_predict_test = model.predict(X_testtf)\ncm = confusion_matrix(y_testtf,y_predict_test)\nsns.heatmap(cm, annot=True, xticklabels = ['Negative', 'Positive'],yticklabels=['Negative','Positive'])\nprint(classification_report(y_testtf,y_predict_test))","metadata":{"id":"mBqQ4J9jK-6B","outputId":"0b252b74-ae6a-4712-847c-e199d1b97c0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import NuSVC\nnusvm = NuSVC(probability=True, kernel='poly', degree=1)","metadata":{"id":"8Ywou2ZjDNPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nusvm.fit(X_train_tfidf2,y)","metadata":{"id":"0au9SL04Ekl0","outputId":"78701226-d7c2-443d-cb0a-61b37b85a88e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\ny_predict_test = model.predict(X_testtf2)\ncm = confusion_matrix(y_testtf2,y_predict_test)\nsns.heatmap(cm, annot=True, xticklabels = ['Negative', 'Positive'],yticklabels=['Negative','Positive'])\nprint(classification_report(y_testtf2,y_predict_test))","metadata":{"id":"kjXV2_XuEr8N","outputId":"b5192bcd-fb23-45c3-a080-23f0d5653580"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=nusvm.predict(X_val_tfidf2)","metadata":{"id":"q_f9sUGsU7nc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"id":"nS0yRzHzVMuP","outputId":"5d3fc731-37c9-4c46-9302-fc0c4ddc95b9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_list= [i for i in range(1,241)]\ndf = pd.DataFrame({'ID':id_list , 'label':labels })\ndf.to_excel('/content/Lamgarraj.xlsx',index=False)","metadata":{"id":"wB4TQvV-V2_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_xls = pd.read_excel('/content/Lamgarraj.xlsx', index_col=None)\ndata_xls.to_csv('/content/LamgarrajZaki.csv', encoding='utf-8',index=False)","metadata":{"id":"IhGyPlgiWRT-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Vote Majoritaire**","metadata":{"id":"7gs5_JUaaEb5"}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\neclf = VotingClassifier(estimators=[\n        ('svm', svm1),('adaboost',ada),('svc_model', RForest),('cnb',model_cnb),(\"sdg\",sgdmodel),(\"NB_classifier\",NB_classifier)], voting='soft')\neclf = eclf.fit(X_train_tfidf2, y)\n","metadata":{"id":"YZPzzy98aJBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"votemajlabels= eclf.predict(X_val_tfidf2)","metadata":{"id":"hjLMoS1hgXHC","outputId":"507e9c85-5954-484a-e864-575e278b1888"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"votemajlabels","metadata":{"id":"-z91JOeGgiFo","outputId":"6907b15d-822e-4de9-f1d9-3044c7804ade"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nclf = VotingClassifier(\n          estimators=[('lr',LogisticRegression()), ('gboost',GradientBoostingClassifier()),]\n          , voting='soft')","metadata":{"id":"0nWZhSRulK40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = [{'lr__C':[1,2],'gboost__n_estimator':[10,20]}]\n\ngrid = GridSearchCV(clf,p,cv=5,scoring='neg_log_loss')\ngrid.fit(X_train_tfidf2,y)","metadata":{"id":"5j2uOnSJlcxb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Complement NB naive bayes**","metadata":{"id":"ZhQRwlyxsW55"}},{"cell_type":"code","source":"from sklearn.naive_bayes import ComplementNB\nfrom sklearn import metrics\nmodel_cnb = ComplementNB()\n\nmodel_cnb.fit(X_train_tfidf, y)\ny_pred_cnb = model_cnb.predict(X_testtf)\n\n\nprint(metrics.classification_report(y_testtf, y_pred_cnb))","metadata":{"id":"9Syz0pScsahA","outputId":"deb07875-9c43-4d70-a8a7-28b720f9ba47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred= model_cnb.predict(X_val_tfidf)","metadata":{"id":"0yE7oG5JszjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"id":"3i0WR7v4ysRx","outputId":"48877d96-d2d9-4e84-8a87-beb468f86dfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l=[]\nfor i in range(120):\n  l.append(1)\nfor i in range(120):\n  l.append(0)","metadata":{"id":"3Wm6jmaEvF6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.classification_report(y_pred, l))","metadata":{"id":"kOMzwFwcvq8m","outputId":"e33f670e-b64b-4d30-d957-8c18f1495d77"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Save and read model**","metadata":{"id":"m8y0x-l3fACy"}},{"cell_type":"code","source":"import pickle\nfilename = '/content/drive/MyDrive/WeBMining/SVM2.sav'\npickle.dump(svm1, open(filename, 'wb'))","metadata":{"id":"MhxEGnay2yHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = pickle.load(open('/content/drive/MyDrive/WeBMining/NBmodel.sav', 'rb'))\nresult = loaded_model.score(X_test, Y_test)\nprint(result)","metadata":{"id":"W0Z70zoR7Fxa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8-S4rBLj7Fq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ynew=np.array(y_pred)","metadata":{"id":"juMbo7ygqc4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"DnspV3rfrHFu","outputId":"1e83ab80-8130-4831-ca43-9e190baea2f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_list = list(range(1,241))\ndf = pd.DataFrame({'ID':id_list , 'label':labels })\ndf.to_excel('/content/Lamgarraj.xlsx',index=False)","metadata":{"id":"OMo8WvHvzCmR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_xls = pd.read_excel('/content/Lamgarraj.xlsx', index_col=None)\ndata_xls.to_csv('/content/LamgarrajZaki.csv', encoding='utf-8',index=False)","metadata":{"id":"pxKiVA6tt0iO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RAndom forest**","metadata":{"id":"C6F6ptNav-OQ"}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# define models and parameters\nmodel = RandomForestClassifier()\nn_estimators = [10, 100,300]\nmax_features = ['auto','sqrt', 'log2']\n# define grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train_tfidf2, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"id":"gdAbm_wWwBPc","outputId":"76d3e156-87ab-4251-ebdd-8757e0126c07"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRForest = RandomForestClassifier(max_features='sqrt', n_estimators= 100)","metadata":{"id":"O0E4G-uFh_n5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RForest.fit(X_train_tfidf2,y)","metadata":{"id":"4TsMCRtBiOow","outputId":"33a904a2-7998-403d-bf38-6e024082c9f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rflabels=RForest.predict(X_val_tfidf2)","metadata":{"id":"eJMYDqXktfHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rflabels","metadata":{"id":"qhDMQMuttpSj","outputId":"fc7b2fe0-3821-40a8-8ae2-8ffd51637196"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\ny_predict_test = RForest.predict(X_testtf)\ncm = confusion_matrix(y_testtf,y_predict_test)\nsns.heatmap(cm, annot=True, xticklabels = ['Negative', 'Positive'],yticklabels=['Negative','Positive'])\nprint(classification_report(y_testtf,y_predict_test))","metadata":{"id":"cfycQSrxiayY","outputId":"bcc17314-fd9b-4d60-da12-ba7cec83582e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=model.predict(X_val_tfidf)","metadata":{"id":"gdzro5tgkZ6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_list = list(range(1,241))\ndf = pd.DataFrame({'ID':id_list , 'label':y_pred })\ndf.to_excel('/content/Lamgarraj.xlsx',index=False)","metadata":{"id":"RXoO6vbdkZMZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_xls = pd.read_excel('/content/Lamgarraj.xlsx', index_col=None)\ndata_xls.to_csv('/content/LamgarrajZaki.csv', encoding='utf-8',index=False)","metadata":{"id":"-QDrVzhilryd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CHI2**","metadata":{"id":"erd9l8uM3v9M"}},{"cell_type":"code","source":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\ncc=SelectKBest(chi2, k=10000)\n\nf_train = cc.fit_transform(X_train_tfidf2, y)\nf_test = cc.transform(X_val_tfidf2)","metadata":{"id":"jgmAJQ8r3yah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_train.shape","metadata":{"id":"Ng3Nphw_4-1R","outputId":"581d36d0-076b-4587-fd82-a5d715ed60f0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Adaboost**","metadata":{"id":"JX9xUR7zp1Yq"}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\n# define the model\nmodel = AdaBoostClassifier()\n","metadata":{"id":"rILVZlJtp4VU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada = AdaBoostClassifier(SVC(probability=True,kernel='linear'))#","metadata":{"id":"nJNIUJFJWcku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada.fit(X_train_tfidf2,y)","metadata":{"id":"RcByoQbevMJH","outputId":"e44916fb-e9de-486f-a473-66f47ec23084"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada2 = AdaBoostClassifier(SGDClassifier(loss='hinge',alpha= 1e-05, l1_ratio= 0.05), algorithm='SAMME')#alpha= 1e-05, l1_ratio= 0.05","metadata":{"id":"t1Fd0d8LZcUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada2.fit(X_train_tfidf2,y)","metadata":{"id":"qKC3cx95ZgRR","outputId":"9890758f-721d-4523-d5ed-ae32cb608562"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adalabels=ada.predict(X_val_tfidf2)","metadata":{"id":"0ZaBX_TJvh_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adalabels","metadata":{"id":"yeRiBD0HZ5NZ","outputId":"3c56223c-fad4-4319-ce9a-0a44b216e3cb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LOGistic Regression**","metadata":{"id":"TeeU8Cylztj_"}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train_tfidf2, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"id":"B3HmMxcyzw97","outputId":"22a1cb8f-3766-423a-90e2-5cf9e287edf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y)","metadata":{"id":"WYdGG55AmzkC","outputId":"d8e7dd64-26a4-4b7d-b9b6-5598f694de38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logm = LogisticRegression(C= 100, penalty= 'l2', solver= 'liblinear')\nlogm.fit(X_train_tfidf1,y)","metadata":{"id":"OulXWq2g1Txq","outputId":"28c86539-5308-414d-bc5e-3ca42a750159"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loglabel=logm.predict(X_val_tfidf1)","metadata":{"id":"StQqPU5C13E1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loglabel","metadata":{"id":"27qEEi-c1yov","outputId":"c937a375-6596-4fd6-ebb8-69337d71d9b6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ynew=np.array(y_pred)","metadata":{"id":"ES0U3peQ1KWL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CNN1**","metadata":{"id":"8etUsVyt6osS"}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses","metadata":{"id":"H0TRKntF7xPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 16","metadata":{"id":"GlMf59ok6qIC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n  layers.Embedding(1000 + 1, embedding_dim),\n  layers.Dropout(0.2),\n  layers.GlobalAveragePooling1D(),\n  layers.Dropout(0.2),\n  layers.Dense(1)])\n\nmodel.summary()","metadata":{"id":"gLZgwe636rAF","outputId":"09ea93fd-2ea2-4895-8804-86b33d494024"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n              optimizer='adam',\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.0001))","metadata":{"id":"Zu7QQwd77EDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf2.shape","metadata":{"id":"UjM6aVG3h9OS","outputId":"a459f9b6-af48-4ff8-c514-54b18cf7decc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train=np.array(y)","metadata":{"id":"vxHiAfxHiyQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\nhistory = model.fit(\n    X_train_tfidf2,\n    y_train,\n    epoch=epochs)","metadata":{"id":"pJf2QVMv7HKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\ny_predict_test = model.predict(X_test)\n","metadata":{"id":"K1RGYbWz7Pxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_test)","metadata":{"id":"nryfozyop_wG","outputId":"19c98ca7-1dfc-42b6-8574-15fc98eb52ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_predict_test)","metadata":{"id":"gzgwz8aUpdK4","outputId":"81866e3e-dd6e-477f-b2f2-d620be85a266"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test,y_predict_test)\nsns.heatmap(cm, annot=True, xticklabels = ['Negative', 'Positive'],yticklabels=['Negative','Positive'])\nprint(classification_report(y_test,y_predict_test))","metadata":{"id":"hHvrwESWos3o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Deep Learning**","metadata":{"id":"qJQZBWvqzyFC"}},{"cell_type":"code","source":"def ensemble_CNN_BiGRU(filters = 100, kernel_size = 3, activation='relu', \n                   input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n  \n    # Channel 1D CNN\n    input1 = tf.keras.Input(shape=(max_length,1420))\n    embeddding1 = tf.keras.layers.Embedding(input_dim=input_dim, \n                            output_dim=output_dim, \n                            input_length=max_length, \n                            input_shape=(max_length, ),\n                            # Assign the embedding weight with word2vec embedding marix\n                            weights = [emb_matrix],\n                            # Set the weight to be not trainable (static)\n                            trainable = False)(input1)\n    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', \n                   kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n    flat1 = Flatten()(pool1)\n    drop1 = Dropout(0.5)(flat1)\n    dense1 = Dense(10, activation='relu')(drop1)\n    drop1 = Dropout(0.5)(dense1)\n    out1 = Dense(1, activation='sigmoid')(drop1)\n    \n    # Channel BiGRU\n    input2 = Input(shape=(max_length,))\n    embeddding2 = Embedding(input_dim=input_dim, \n                            output_dim=output_dim, \n                            input_length=max_length, \n                            input_shape=(max_length, ),\n                            # Assign the embedding weight with word2vec embedding marix\n                            weights = [emb_matrix],\n                            # Set the weight to be not trainable (static)\n                            trainable = False,\n                            mask_zero=True)(input2)\n    gru2 = Bidirectional(GRU(64))(embeddding2)\n    drop2 = Dropout(0.5)(gru2)\n    out2 = Dense(1, activation='sigmoid')(drop2)\n    \n    # Merge\n    merged = concatenate([out1, out2])\n    \n    # Interpretation\n    outputs = Dense(1, activation='sigmoid')(merged)\n    model = Model(inputs=[input1, input2], outputs=outputs)\n    \n    # Compile\n    model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","metadata":{"id":"diUko3Fz-zfz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=ensemble_CNN_BiGRU(filters = 100, kernel_size = 3, activation='relu', \n                   input_dim = None, output_dim=300, max_length = None, emb_matrix = None)","metadata":{"id":"pg3QckU4_ok1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DEEP3","metadata":{"id":"tNNlBDYv-8kW"}},{"cell_type":"code","source":"def tcn_model(kernel_size = 3, activation='relu', input_dim = None, \n                   output_dim=300, max_length = None, emb_matrix = None):\n    \n    inp = Input( shape=(max_length,))\n    x = Embedding(input_dim=input_dim, \n                  output_dim=output_dim, \n                  input_length=max_length,\n                  # Assign the embedding weight with word2vec embedding marix\n                  weights = [emb_matrix],\n                  # Set the weight to be not trainable (static)\n                  trainable = False)(inp)\n    \n    x = SpatialDropout1D(0.1)(x)\n    \n    x = TCN(128,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn1')(x)\n    x = TCN(64,dilations = [1, 2, 4], return_sequences=True, activation = activation, name = 'tcn2')(x)\n    \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    \n    conc = concatenate([avg_pool, max_pool])\n    conc = Dense(16, activation=\"relu\")(conc)\n    conc = Dropout(0.1)(conc)\n    outp = Dense(1, activation=\"sigmoid\")(conc)    \n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    \n    return model","metadata":{"id":"MVxm0bKL_IPq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape[0]","metadata":{"id":"qPMLFGtZvXYg","outputId":"f387bfd1-129e-404e-cc97-ee93070fa134"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4CNN**","metadata":{"id":"F38mlB5TpLtn"}},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\ninput_shape = (28,28, 1)\ncnn4 = Sequential()\ncnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\ncnn4.add(BatchNormalization())\n\ncnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(MaxPooling2D(pool_size=(2, 2)))\ncnn4.add(Dropout(0.25))\n\ncnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(Dropout(0.25))\n\ncnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(MaxPooling2D(pool_size=(2, 2)))\ncnn4.add(Dropout(0.25))\n\ncnn4.add(Flatten())\n\ncnn4.add(Dense(512, activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(Dropout(0.5))\n\ncnn4.add(Dense(128, activation='relu'))\ncnn4.add(BatchNormalization())\ncnn4.add(Dropout(0.5))\n\ncnn4.add(Dense(10, activation='softmax'))","metadata":{"id":"UWVr8Vb9pOUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn4.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"id":"DicuSLwgqMZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn4.summary()","metadata":{"id":"hu1iDM4zqRrc","outputId":"7c5a2144-1134-4acf-99f4-030e95c9ccdb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"id":"oqJTRh2hyMqN","outputId":"cf51c3b6-5fd1-4545-dcc0-59935b06f2f7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_train = X_train.astype('float32')\nX_train/=255","metadata":{"id":"Ix-DPhT3zBfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"id":"2AKXHXZ6zGKB","outputId":"ea5bfcb7-abb7-48ab-94d7-30236546dd5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"id":"Hf0PoCJZ0BS5","outputId":"f486fad7-5bcd-4ef2-b485-f36f2c272cbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history4 = cnn4.fit(X_train, y_train,\n          batch_size=256,\n          epochs=10,\n          verbose=1\n          )","metadata":{"id":"c98b1Tb2qaAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score4 = cnn4.evaluate(X_predict, y_test, verbose=0)\nprint('Test loss:', score4[0])\nprint('Test accuracy:', score4[1])","metadata":{"id":"Bq_6_mu1qhdt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **tfidf2**","metadata":{"id":"LYN8P8nLWeI1"}},{"cell_type":"code","source":"\nfrom gensim import corpora, models\n\nclass TFIDF:\n    \n    def __init__(self, documents):\n        self.tokenized_document = [[token for token in document.lower().split()] for document in documents]\n        self.dictionary = corpora.Dictionary(self.tokenized_document)\n        self.corpus_doc2bow_vectors = [self.dictionary.doc2bow(tok_doc) for tok_doc in self.tokenized_document]\n        self.tfidf_model = models.TfidfModel(self.corpus_doc2bow_vectors, id2word=self.dictionary, normalize=False)\n        self.corpus_tfidf_vectors = self.tfidf_model[self.corpus_doc2bow_vectors]\n        ","metadata":{"id":"EnWI9r3jWdKl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom gensim import similarities\nfrom prettytable import PrettyTable\nquery_bow_vector=[]\nfor comment in data[\"comment\"] :\n     tf_idf = TFIDF(comment)\n     corpus_tfidf_vectors = tf_idf.corpus_tfidf_vectors\n     dictionary = tf_idf.dictionary\n     tfidf_model = tf_idf.tfidf_model\n     query_bow_vector.append(dictionary.doc2bow(process_text(comment)))\n     query_tfidf_vector = tfidf_model[query_bow_vector]","metadata":{"id":"J2KEYShTW85S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_bow_vector","metadata":{"id":"j6ssb9fMTzpS","outputId":"8751a95e-6189-416b-96fc-70b748138594"},"execution_count":null,"outputs":[]}]}